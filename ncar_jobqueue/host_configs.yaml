hobart:
  pbs:
    name: dask-worker
    cores: 48                  # Total number of cores per job
    memory: '96GB'             # Total amount of memory per job
    processes: 3                # Number of Python processes per job
#    interface: null              # Network interface to use like eth0 or ib0
    queue: medium
    walltime: '08:00:00'
    resource-spec: nodes=1:ppn=48
    job-extra: ["-r n"]
    local-directory: "/scratch/cluster/${USER}/dask-tmp"

cheyenne_casper:
  pbs:
#    project: NCGD0011
    name: dask-worker
    cores: 36                   # Total number of cores per job
    memory: '109GB'             # Total amount of memory per job
    processes: 3                # Number of Python processes per job
    interface: ib0              # Network interface to use like eth0 or ib0
    queue: regular
    walltime: '04:00:00'
    resource-spec: select=1:ncpus=36:mem=109GB
    job-extra: ["-o /glade/scratch/${USER}/", "-e /glade/scratch/${USER}/"]
    local-directory: "/glade/scratch/${USER}/dask-tmp"

  slurm:
#    project: NCGD0011
    name: dask-worker
    cores: 1                    # Total number of cores per job
    memory: '25GB'              # Total amount of memory per job
    processes: 1                # Number of Python processes per job
    interface: ib0
    walltime: '06:00:00'
    job-extra: ["-C casper", "-o /glade/scratch/${USER}/dask-worker.o%J", "-e /glade/scratch/${USER}/dask-worker.e%J"]
    local-directory: /glade/scratch/${USER}/dask-tmp
